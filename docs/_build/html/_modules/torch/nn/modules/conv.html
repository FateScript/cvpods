

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>torch.nn.modules.conv &mdash; cvpods 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> cvpods
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/index.html">Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules/index.html">API Documentation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">cvpods</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>torch.nn.modules.conv</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for torch.nn.modules.conv</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="k">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.nn.parameter</span> <span class="k">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="k">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="k">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="k">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="k">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_reverse_repeat_tuple</span>

<span class="kn">from</span> <span class="nn">..common_types</span> <span class="k">import</span> <span class="n">_size_1_t</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">,</span> <span class="n">_size_3_t</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>


<span class="k">class</span> <span class="nc">_ConvNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="s1">&#39;groups&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="s1">&#39;in_channels&#39;</span><span class="p">,</span>
                     <span class="s1">&#39;out_channels&#39;</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">]</span>
    <span class="vm">__annotations__</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;bias&#39;</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]}</span>

    <span class="n">_in_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">output_padding</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
                 <span class="n">transposed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                 <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;in_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="n">valid_padding_modes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="s1">&#39;replicate&#39;</span><span class="p">,</span> <span class="s1">&#39;circular&#39;</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_padding_modes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding_mode must be one of </span><span class="si">{}</span><span class="s2">, but got padding_mode=&#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">valid_padding_modes</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="n">transposed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">output_padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="c1"># `_reversed_padding_repeated_twice` is the padding to be passed to</span>
        <span class="c1"># `F.pad` if needed (e.g., for non-zero padding types that are</span>
        <span class="c1"># implemented as two ops: padding + conv). `F.pad` accepts paddings in</span>
        <span class="c1"># reverse order than the dimension.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span> <span class="o">=</span> <span class="n">_reverse_repeat_tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
            <span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{in_channels}</span><span class="s1">, </span><span class="si">{out_channels}</span><span class="s1">, kernel_size=</span><span class="si">{kernel_size}</span><span class="s1">&#39;</span>
             <span class="s1">&#39;, stride=</span><span class="si">{stride}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding=</span><span class="si">{padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, dilation=</span><span class="si">{dilation}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, output_padding=</span><span class="si">{output_padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, groups=</span><span class="si">{groups}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, bias=False&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding_mode=</span><span class="si">{padding_mode}</span><span class="s1">&#39;</span>
        <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;padding_mode&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>


<span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    In the simplest case, the output value of the layer with input size</span>
<span class="sd">    :math:`(N, C_{\text{in}}, L)` and output :math:`(N, C_{\text{out}}, L_{\text{out}})` can be</span>
<span class="sd">    precisely described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)</span>
<span class="sd">        \star \text{input}(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid `cross-correlation`_ operator,</span>
<span class="sd">    :math:`N` is a batch size, :math:`C` denotes a number of channels,</span>
<span class="sd">    :math:`L` is a length of signal sequence.</span>

<span class="sd">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation, a single</span>
<span class="sd">      number or a one-element tuple.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both sides</span>
<span class="sd">      for :attr:`padding` number of points.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also</span>
<span class="sd">      known as the à trous algorithm. It is harder to describe, but this `link`_</span>
<span class="sd">      has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters,</span>
<span class="sd">          of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.</span>

<span class="sd">    Note:</span>

<span class="sd">        Depending of the size of your kernel, several (of the last)</span>
<span class="sd">        columns of the input might be lost, because it is a valid</span>
<span class="sd">        `cross-correlation`_, and not a full `cross-correlation`_.</span>
<span class="sd">        It is up to the user to add proper padding.</span>

<span class="sd">    Note:</span>

<span class="sd">        When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">        where `K` is a positive integer, this operation is also termed in</span>
<span class="sd">        literature as depthwise convolution.</span>

<span class="sd">        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,</span>
<span class="sd">        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments</span>
<span class="sd">        :math:`(C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})`.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of</span>
<span class="sd">            the input. Default: 0</span>
<span class="sd">        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="sd">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel</span>
<span class="sd">            elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input</span>
<span class="sd">            channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="sd">            output. Default: ``True``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, L_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, L_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}</span>
<span class="sd">                        \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">            :math:`(\text{out\_channels},</span>
<span class="sd">            \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})`.</span>
<span class="sd">            The values of these weights are sampled from</span>
<span class="sd">            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape</span>
<span class="sd">            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>  <span class="c1"># TODO: refine this type</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                            <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    In the simplest case, the output value of the layer with input size</span>
<span class="sd">    :math:`(N, C_{\text{in}}, H, W)` and output :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})`</span>
<span class="sd">    can be precisely described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</span>


<span class="sd">    where :math:`\star` is the valid 2D `cross-correlation`_ operator,</span>
<span class="sd">    :math:`N` is a batch size, :math:`C` denotes a number of channels,</span>
<span class="sd">    :math:`H` is a height of input planes in pixels, and :math:`W` is</span>
<span class="sd">    width in pixels.</span>

<span class="sd">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation, a single</span>
<span class="sd">      number or a tuple.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for :attr:`padding` number of points for each dimension.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also</span>
<span class="sd">      known as the à trous algorithm. It is harder to describe, but this `link`_</span>
<span class="sd">      has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters, of size:</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the height and width dimension</span>
<span class="sd">        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,</span>
<span class="sd">          and the second `int` for the width dimension</span>

<span class="sd">    Note:</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    Note:</span>

<span class="sd">        When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">        where `K` is a positive integer, this operation is also termed in</span>
<span class="sd">        literature as depthwise convolution.</span>

<span class="sd">        In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments</span>
<span class="sd">        :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to both sides of</span>
<span class="sd">            the input. Default: 0</span>
<span class="sd">        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,</span>
<span class="sd">            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input</span>
<span class="sd">            channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the</span>
<span class="sd">            output. Default: ``True``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]</span>
<span class="sd">                        \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]</span>
<span class="sd">                        \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">            :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`</span>
<span class="sd">            :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.</span>
<span class="sd">            The values of these weights are sampled from</span>
<span class="sd">            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape</span>
<span class="sd">            (out_channels). If :attr:`bias` is ``True``,</span>
<span class="sd">            then the values of these weights are</span>
<span class="sd">            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>  <span class="c1"># TODO: refine this type</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                            <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D convolution over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`</span>
<span class="sd">    and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        out(N_i, C_{out_j}) = bias(C_{out_j}) +</span>
<span class="sd">                                \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid 3D `cross-correlation`_ operator</span>

<span class="sd">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for :attr:`padding` number of points for each dimension.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters, of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension</span>
<span class="sd">        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,</span>
<span class="sd">          the second `int` for the height dimension and the third `int` for the width dimension</span>

<span class="sd">    Note:</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    Note:</span>

<span class="sd">        When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">        where `K` is a positive integer, this operation is also termed in</span>
<span class="sd">        literature as depthwise convolution.</span>

<span class="sd">        In other words, for an input of size :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`,</span>
<span class="sd">        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments</span>
<span class="sd">        :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0</span>
<span class="sd">        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]</span>
<span class="sd">                    \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]</span>
<span class="sd">                    \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]</span>
<span class="sd">                    \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,</span>
<span class="sd">                         then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_3_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ConvTransposeNd</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only &quot;zeros&quot; padding mode is supported for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="c1"># dilation being an optional parameter is for backwards</span>
    <span class="c1"># compatibility</span>
    <span class="k">def</span> <span class="nf">_output_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># type: (Tensor, Optional[List[int]], List[int], List[int], List[int], Optional[List[int]]) -&gt; List[int]</span>
        <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">)</span>  <span class="c1"># converting to list if was not already</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">k</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;output_size must have </span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements (got </span><span class="si">{}</span><span class="s2">)&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>

            <span class="n">min_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="n">max_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">dim_size</span> <span class="o">=</span> <span class="p">((</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span>
                            <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="p">(</span><span class="n">dilation</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="k">if</span> <span class="n">dilation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">min_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim_size</span><span class="p">)</span>
                <span class="n">max_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">min_sizes</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)):</span>
                <span class="n">size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">min_size</span> <span class="o">=</span> <span class="n">min_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">max_size</span> <span class="o">=</span> <span class="n">max_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;</span> <span class="n">min_size</span> <span class="ow">or</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="n">max_size</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
                        <span class="s2">&quot;requested an output size of </span><span class="si">{}</span><span class="s2">, but valid sizes range &quot;</span>
                        <span class="s2">&quot;from </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{}</span><span class="s2"> (for an input of </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">output_size</span><span class="p">,</span> <span class="n">min_sizes</span><span class="p">,</span> <span class="n">max_sizes</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]))</span>

            <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">min_sizes</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">ret</span>


<span class="k">class</span> <span class="nc">ConvTranspose1d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes.</span>

<span class="sd">    This module can be seen as the gradient of Conv1d with respect to its input.</span>
<span class="sd">    It is also known as a fractionally-strided convolution or</span>
<span class="sd">    a deconvolution (although it is not an actual deconvolution operation).</span>

<span class="sd">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="sd">      below for details.</span>

<span class="sd">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="sd">      of the output shape. See note below for details.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters (of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).</span>

<span class="sd">    Note:</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    Note:</span>
<span class="sd">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="sd">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="sd">        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`</span>
<span class="sd">        are initialized with same parameters, they are inverses of each other in</span>
<span class="sd">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="sd">        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output</span>
<span class="sd">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="sd">        effectively increasing the calculated output shape on one side. Note</span>
<span class="sd">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="sd">        not actually add zero-padding to output.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, L_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, L_{out})` where</span>

<span class="sd">          .. math::</span>
<span class="sd">              L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}</span>
<span class="sd">                        \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels).</span>
<span class="sd">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_1_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_1_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose1d&#39;</span><span class="p">)</span>

        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>


<div class="viewcode-block" id="ConvTranspose2d"><a class="viewcode-back" href="../../../../modules/layers.html#cvpods.layers.ConvTranspose2d">[docs]</a><span class="k">class</span> <span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes.</span>

<span class="sd">    This module can be seen as the gradient of Conv2d with respect to its input.</span>
<span class="sd">    It is also known as a fractionally-strided convolution or</span>
<span class="sd">    a deconvolution (although it is not an actual deconvolution operation).</span>

<span class="sd">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="sd">      below for details.</span>

<span class="sd">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="sd">      of the output shape. See note below for details.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters (of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`</span>
<span class="sd">    can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the height and width dimensions</span>
<span class="sd">        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,</span>
<span class="sd">          and the second `int` for the width dimension</span>

<span class="sd">    .. note::</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    Note:</span>
<span class="sd">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="sd">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="sd">        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`</span>
<span class="sd">        are initialized with same parameters, they are inverses of each other in</span>
<span class="sd">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="sd">        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output</span>
<span class="sd">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="sd">        effectively increasing the calculated output shape on one side. Note</span>
<span class="sd">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="sd">        not actually add zero-padding to output.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of each dimension in the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where</span>

<span class="sd">        .. math::</span>
<span class="sd">              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]</span>
<span class="sd">                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</span>
<span class="sd">        .. math::</span>
<span class="sd">              W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]</span>
<span class="sd">                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels)</span>
<span class="sd">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>
<span class="sd">        &gt;&gt;&gt; # exact output size can be also specified as an argument</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)</span>
<span class="sd">        &gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; h = downsample(input)</span>
<span class="sd">        &gt;&gt;&gt; h.size()</span>
<span class="sd">        torch.Size([1, 16, 6, 6])</span>
<span class="sd">        &gt;&gt;&gt; output = upsample(h, output_size=input.size())</span>
<span class="sd">        &gt;&gt;&gt; output.size()</span>
<span class="sd">        torch.Size([1, 16, 12, 12])</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

<div class="viewcode-block" id="ConvTranspose2d.forward"><a class="viewcode-back" href="../../../../modules/layers.html#cvpods.layers.ConvTranspose2d.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose2d&#39;</span><span class="p">)</span>

        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">ConvTranspose3d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D transposed convolution operator over an input image composed of several input</span>
<span class="sd">    planes.</span>
<span class="sd">    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,</span>
<span class="sd">    and sums over the outputs from all input feature planes.</span>

<span class="sd">    This module can be seen as the gradient of Conv3d with respect to its input.</span>
<span class="sd">    It is also known as a fractionally-strided convolution or</span>
<span class="sd">    a deconvolution (although it is not an actual deconvolution operation).</span>

<span class="sd">    This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    * :attr:`stride` controls the stride for the cross-correlation.</span>

<span class="sd">    * :attr:`padding` controls the amount of implicit zero-paddings on both</span>
<span class="sd">      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note</span>
<span class="sd">      below for details.</span>

<span class="sd">    * :attr:`output_padding` controls the additional size added to one side</span>
<span class="sd">      of the output shape. See note below for details.</span>

<span class="sd">    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.</span>
<span class="sd">      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.</span>

<span class="sd">    * :attr:`groups` controls the connections between inputs and outputs.</span>
<span class="sd">      :attr:`in_channels` and :attr:`out_channels` must both be divisible by</span>
<span class="sd">      :attr:`groups`. For example,</span>

<span class="sd">        * At groups=1, all inputs are convolved to all outputs.</span>
<span class="sd">        * At groups=2, the operation becomes equivalent to having two conv</span>
<span class="sd">          layers side by side, each seeing half the input channels,</span>
<span class="sd">          and producing half the output channels, and both subsequently</span>
<span class="sd">          concatenated.</span>
<span class="sd">        * At groups= :attr:`in_channels`, each input channel is convolved with</span>
<span class="sd">          its own set of filters (of size</span>
<span class="sd">          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).</span>

<span class="sd">    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`</span>
<span class="sd">    can either be:</span>

<span class="sd">        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions</span>
<span class="sd">        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,</span>
<span class="sd">          the second `int` for the height dimension and the third `int` for the width dimension</span>

<span class="sd">    Note:</span>

<span class="sd">         Depending of the size of your kernel, several (of the last)</span>
<span class="sd">         columns of the input might be lost, because it is a valid `cross-correlation`_,</span>
<span class="sd">         and not a full `cross-correlation`_.</span>
<span class="sd">         It is up to the user to add proper padding.</span>

<span class="sd">    Note:</span>
<span class="sd">        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``</span>
<span class="sd">        amount of zero padding to both sizes of the input. This is set so that</span>
<span class="sd">        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`</span>
<span class="sd">        are initialized with same parameters, they are inverses of each other in</span>
<span class="sd">        regard to the input and output shapes. However, when ``stride &gt; 1``,</span>
<span class="sd">        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output</span>
<span class="sd">        shape. :attr:`output_padding` is provided to resolve this ambiguity by</span>
<span class="sd">        effectively increasing the calculated output shape on one side. Note</span>
<span class="sd">        that :attr:`output_padding` is only used to find output shape, but does</span>
<span class="sd">        not actually add zero-padding to output.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of channels in the input image</span>
<span class="sd">        out_channels (int): Number of channels produced by the convolution</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolving kernel</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default: 1</span>
<span class="sd">        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding</span>
<span class="sd">            will be added to both sides of each dimension in the input. Default: 0</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to one side</span>
<span class="sd">            of each dimension in the output shape. Default: 0</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1</span>
<span class="sd">        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``</span>
<span class="sd">        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`</span>
<span class="sd">        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where</span>

<span class="sd">        .. math::</span>
<span class="sd">              D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]</span>
<span class="sd">                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</span>
<span class="sd">        .. math::</span>
<span class="sd">              H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]</span>
<span class="sd">                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</span>
<span class="sd">        .. math::</span>
<span class="sd">              W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]</span>
<span class="sd">                        \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1</span>


<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor): the learnable weights of the module of shape</span>
<span class="sd">                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`</span>
<span class="sd">                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.</span>
<span class="sd">                         The values of these weights are sampled from</span>
<span class="sd">                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>
<span class="sd">        bias (Tensor):   the learnable bias of the module of shape (out_channels)</span>
<span class="sd">                         If :attr:`bias` is ``True``, then the values of these weights are</span>
<span class="sd">                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where</span>
<span class="sd">                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; output = m(input)</span>

<span class="sd">    .. _cross-correlation:</span>
<span class="sd">        https://en.wikipedia.org/wiki/Cross-correlation</span>

<span class="sd">    .. _link:</span>
<span class="sd">        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_3_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_3_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only `zeros` padding mode is supported for ConvTranspose3d&#39;</span><span class="p">)</span>

        <span class="n">output_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_padding</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>


<span class="c1"># TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.</span>
<span class="c1">#</span>
<span class="c1"># `_ConvTransposeMixin` was a mixin that was removed.  It is meant to be used</span>
<span class="c1"># with `_ConvNd` to construct actual module classes that implements conv</span>
<span class="c1"># transpose ops:</span>
<span class="c1">#</span>
<span class="c1">#   class MyConvTranspose(_ConvNd, _ConvTransposeMixin):</span>
<span class="c1">#       ...</span>
<span class="c1">#</span>
<span class="c1"># In PyTorch, it has been replaced by `_ConvTransposeNd`, which is a proper</span>
<span class="c1"># subclass of `_ConvNd`.  However, some user code in the wild still (incorrectly)</span>
<span class="c1"># use the internal class `_ConvTransposeMixin`.  Hence, we provide this alias</span>
<span class="c1"># for BC, because it is cheap and easy for us to do so, even though that</span>
<span class="c1"># `_ConvTransposeNd` is really not a mixin anymore (but multiple inheritance as</span>
<span class="c1"># above would still work).</span>
<span class="k">class</span> <span class="nc">_ConvTransposeMixin</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;_ConvTransposeMixin is a deprecated internal class. &quot;</span>
            <span class="s2">&quot;Please consider using public APIs.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvTransposeMixin</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="c1"># TODO: Conv2dLocal</span>
<span class="c1"># TODO: Conv2dMap</span>
<span class="c1"># TODO: ConvTranspose2dMap</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2019, cvpods contributors.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>